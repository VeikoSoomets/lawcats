# -*- coding: utf-8 -*-

import sys
sys.path.insert(0, 'libs')
import bs4
import urllib2
import re
from datetime import datetime
import datetime
import time

import logging
import urllib

#from google.appengine.api import urlfetch
#urlfetch.set_default_fetch_deadline(45)

from utils import *

categories = [\
['Delfi arhiiv','http://www.delfi.ee'], \
[u'Äripäev arhiiv','http://www.aripaev.ee'], \
[u'raamatupidaja.ee otsing','http://www.raamatupidaja.ee'], \
[u'Õhtuleht arhiiv','http://www.ohtuleht.ee'], \
[u'Деловное Деломости архив','http://www.dv.ee'], \
[u'МК-Эстония поиск','http://www.mke.ee'], \
]

#import html5lib # have to use this because politsei web has shitty/broken code, bs4 can't handle this shit alone. NB! needs to be installed in libs folder just like bs4

# TODO: try and except (if search fails)
    
def search_media(querywords,category,date_algus):
    #print date_algus
    date_algus=datetime_object(date_algus)
    date_lopp = datetime.datetime.now().date()
    
    results=[]
    for query in querywords:
      query=urllib.quote_plus(query.encode('utf-8'))

      if category=='Delfi arhiiv':
        date_algus_format  = date_algus.strftime('%d.%m.%Y')
        date_lopp_format = date_lopp.strftime('%d.%m.%Y')
        url="http://www.delfi.ee/archive/?tod=" + date_lopp_format + "&fromd=" + date_algus_format + "&channel=0&category=0&query=" + query
        
      elif category=='Postimees arhiiv': # NB! not possible to implement because source content is being generated by javascript
        date_algus_format  =int(time.mktime(date_algus.timetuple())) # unix timestamp
        url="http://www.postimees.ee/search?dateStart=" + date_algus_format + "&dateEnd=0&section=81&sisu=" + query + "&showAdvanced=1"
      
      elif category in [u'Äripäev arhiiv',u'Деловное Деломости архив',u'raamatupidaja.ee otsing']: # sama otsing
        date_algus_format  = date_algus.strftime('%Y-%m-%d')
        date_lopp_format = date_lopp.strftime('%Y-%m-%d')
        if category==u'Äripäev arhiiv':
          urlbase="http://www.aripaev.ee/"
        elif category==u'Деловное Деломости архив':
          urlbase="http://dv.ee/"
        elif category==u'raamatupidaja.ee otsing':
          urlbase="http://www.raamatupidaja.ee/"
        url = urlbase + "search?q=" + query + "&p=6&period=" + date_algus_format + "Z," + date_lopp_format + "Z"
      
      elif category==u'Õhtuleht arhiiv':
        date_algus_format  = date_algus.strftime('%Y-%m-%d')
        date_lopp_format = date_lopp.strftime('%Y-%m-%d')
        url = "http://www.ohtuleht.ee/arhiiv/otsing?q=" + query + "&b=" + date_algus_format + "&e=" + date_lopp_format + "&r="
        
      elif category==u'МК-Эстония поиск':
        url = "http://www.mke.ee/poisk?searchword=" + query + "&searchphrase=all"
      
      # OPEN LINK
      opener = urllib2.build_opener()
      opener.addheaders = [('User-agent', 'Mozilla/5.0')]
      response = opener.open(url)
      #src=urllib2.urlopen(url)
      
      # PARSE LINK
      if category=='Delfi arhiiv':
        results.extend(parse_results_delfi(response,query,category))
        
      if category==u'Õhtuleht arhiiv':
        results.extend(parse_results_ohtuleht(response,query,category))
        
      if category==u'МК-Эстония поиск':
        results.extend(parse_results_mke(response,query,category))
        
      if category in [u'Äripäev arhiiv',u'Деловное Деломости архив',u'raamatupidaja.ee otsing']:
        results.extend(parse_results_standard(response,query,category))
        """ # Use this for testing
        import os
        file_dir = os.path.dirname(os.path.abspath(__file__))
        with open(file_dir+"/aripaev.txt", 'r') as src:
          results.extend(parse_results_aripaev(src,query,category))
          #return parse_results_kohtu(src)  """

    return results
 
def parse_results_standard(src,query=None,category=None):
    
    soup = bs4.BeautifulSoup(src.read(), 'html5lib') 
    
    if category == u'Äripäev arhiiv':
      url_base = 'http://www.aripaev.ee'
      soup = soup.findAll('div', attrs={"class": "art-block"})
      
    elif category == u'Деловное Деломости архив':
      url_base = 'http://dv.ee'
      soup = soup.findAll('div', attrs={"class": "art-block"})
      
    elif category == u'raamatupidaja.ee otsing':
      url_base = 'http://www.raamatupidaja.ee'
      soup = soup.findAll('div', attrs={"class": "main-search-result"})
   
    results=[]
        
    for result in soup:
      cats=[]
      #print len(result)
      if len(result)>=14 or (len(result)>=7 and category == u'raamatupidaja.ee otsing'): # 14 for venekeelne äripäev, 15 for EE äripäev, 7 for raamatupidaja
        link_item=result.findNext('a').parent.find('a')
        title=clear_string2(link_item.get_text())
        link=url_base+clear_string2(link_item.get('href'))
        if category == u'raamatupidaja.ee otsing':
          stringdate=result.find('p',attrs={"class": "result-datetime"}).get_text()
          
          m = re.search("\d", stringdate)
          stringdate=stringdate[m.start():-7] # start searching from first number (because for raamatupidaja, you might have text before date)
        else:
          stringdate=result.find('span',attrs={"class": "art-date"}).get_text()
        date=sql_normalize_date(stringdate)
        
        if len(title)>1 or len(title):
          cats.append(link)
          cats.append(title)
          cats.append(date)
          cats.append(query)
          cats.append(category)
          results.append(cats)
      """  for content in result.findNext('h3'):
          cats=[]
      
          link_item=content.findNext('a') #.parent.find('a')
          title=clear_string2(link_item.get_text())
          #print repr(title)
          link=url_base+clear_string2(link_item.get('href'))
          

          stringdate=result.find('span',attrs={"class": "art-date"}).get_text()
          date=sql_normalize_date(stringdate)
          cats.append(link)
          cats.append(title)
          cats.append(date)
          cats.append(query)
          cats.append(category)
        results.append(cats) """
    
    return results 
    #return results #  link, title, date, qword, category 

def parse_results_delfi(src,query=None,category=None):
    encoding=src.headers['content-type'].split('charset=')[-1] # we have to use this fix or EE letters are causing trouble
    soup = bs4.BeautifulSoup(src, from_encoding=encoding)
    #soup = bs4.BeautifulSoup(src.read(), 'html5lib')
    results=[]
    soup = soup.find('div', attrs={"class": "arch-search-list"})

    for result in soup.findAll('li'):
      cats=[]
      for link in result.findAll('a', attrs={"class":"arArticleT"}): #attrs={"class":"arArticleT"}
        #print index, link
        title=link.get_text()
        link=link.get('href')
        #for item in result.findAll('dt'):  # leiame kuupäeva
      stringdate=result.find('span').get_text()[0:10] #.decode('utf-8') # unicode type, muidu re ei tööta
      date = sql_normalize_date(stringdate)
      #stringdate=clear_string(stringdate)
      #stringdate=sql_normalize_date(stringdate)
      cats.append(link)
      cats.append(title)
      cats.append(date)
      cats.append(query)
      cats.append(category)
      results.append(cats)
    return results
    #return results #  link, title, date, qword, category
    
def parse_results_ohtuleht(src,query=None,category=None):
    soup = bs4.BeautifulSoup(src.read(), 'html5lib') 
    results=[]
    soup = soup.find('ul', attrs={"class": "title-list"})

    for result in soup.findAll('li'):
      cats=[]
      for link in result.findAll('a', href=True):
        #print index, link
        title=link.get_text()
        link=link.get('href')
        #for item in result.findAll('dt'):  # leiame kuupäeva
      stringdate=result.find('div').get_text() #[0:10] #.decode('utf-8') # unicode type, muidu re ei tööta
      
      # get a normal date. INPUT: 07:00, 8. aprill 2015 / Uudised | Eesti uudised; OUTPUT: 8. aprill 2015 (input varies alot)
      stringdate=stringdate[7:]
      year = re.search(r'\d{4}', stringdate).group()
      years_start=stringdate.find(year)
      stringdate=stringdate[:years_start+4]
      
      date = sql_normalize_date(stringdate)
      #stringdate=clear_string(stringdate)
      #stringdate=sql_normalize_date(stringdate)
      cats.append(link)
      cats.append(title)
      cats.append(date)
      cats.append(query)
      cats.append(category)
      results.append(cats)
    return results
    #return results #  link, title, date, qword, category
    
def parse_results_mke(src,query=None,category=None):
    soup = bs4.BeautifulSoup(src.read(), 'html5lib') 
    results=[]
    url_base = 'http://www.mke.ee'
    soup = soup.find('dl', attrs={"class": "search-results"})

    for result in soup.findAll('dt', attrs={"class":"result-title"}):
      cats=[]
      for link in result.findAll('a', href=True):
        #print index, link
        title=link.get_text()
        link=url_base+link.get('href')
        date='' # there is no date for this source
      cats.append(link)
      cats.append(title)
      cats.append(date)
      cats.append(query)
      cats.append(category)
      results.append(cats)
    return results
    #return results #  link, title, date, qword, category
 
# TESTING #####
if __name__ == "__main__":
  results=search_media([u'swedbank'],u'Delfi arhiiv','2015-05-01') 
  for a in results:
    print a, '\n' , '\n' 
  raw_input('press enter to close...')
# ########################